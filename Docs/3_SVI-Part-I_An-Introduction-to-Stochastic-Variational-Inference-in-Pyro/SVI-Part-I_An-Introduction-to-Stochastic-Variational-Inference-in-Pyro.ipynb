{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variational inference (機率圖模型)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "    observations ⟺ pyro.observe\n",
    "    latent random variables ⟺ pyro.sample\n",
    "    parameters ⟺ pyro.param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide (variational distribution)\n",
    "    Introduce a parameterized distribution\n",
    "    1. model() and guide() needs to have a matching sample statement \n",
    "    2. Learning will be setup as an optimization problem where each iteration of training takes a step in  space that moves the guide closer to the exact posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO (evidence lower bound)\n",
    "     At high level variational inference is easy: all we need to do is define a guide and compute gradients of the ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVI Class\n",
    "    In Pyro the machinery for doing variational inference is encapsulated in the SVI class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.infer import SVI\n",
    "# svi = SVI(model, guide, optimizer, loss=\"ELBO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "    Pyro needs to dynamically generate an optimizer for each parameter the first time it appears during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.optim import Adam\n",
    "\n",
    "adam_params = {\"lr\": 0.005, \"betas\": (0.95, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "def per_param_callable(module_name, param_name, tags):\n",
    "    if 'param_name' == 'my_special_parameter':\n",
    "        return {\"lr\": 0.010}\n",
    "    else:\n",
    "        return {\"lr\": 0.001}\n",
    "\n",
    "optimizer = Adam(per_param_callable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example\n",
    "#### We finish with a simple example. You’ve been given a two-sided coin. You want to determine whether the coin is fair or not, i.e. whether it falls heads or tails with the same frequency. You have a prior belief about the likely fairness of the coin based on two observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.distributions as dist\n",
    "\n",
    "def model(data):\n",
    "    # define the hyperparameters that control the beta prior\n",
    "    alpha0 = Variable(torch.Tensor([10.0]))\n",
    "    beta0 = Variable(torch.Tensor([10.0]))\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.beta, alpha0, beta0)\n",
    "    # loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the bernoulli\n",
    "        # likelihood Bernoulli(f)\n",
    "        pyro.observe(\"obs_{}\".format(i), dist.bernoulli,\n",
    "                     data[i], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Our next task is to define a corresponding guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(data):\n",
    "    # define the initial values of the two variational parameters\n",
    "    log_alpha_q_0 = Variable(torch.Tensor([np.log(15.0)]),\n",
    "                             requires_grad=True)\n",
    "    log_beta_q_0 = Variable(torch.Tensor([np.log(15.0)]),\n",
    "                            requires_grad=True)\n",
    "    # register the two variational parameters with Pyro\n",
    "    log_alpha_q = pyro.param(\"log_alpha_q\", log_alpha_q_0)\n",
    "    log_beta_q = pyro.param(\"log_beta_q\", log_beta_q_0)\n",
    "    alpha_q, beta_q = torch.exp(log_alpha_q), torch.exp(log_beta_q)\n",
    "    # sample latent_fairness from the distribution\n",
    "    # Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness\", dist.beta, alpha_q, beta_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the optimizer\n",
    "adam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide, optimizer, loss=\"ELBO\")\n",
    "\n",
    "n_steps = 5000\n",
    "# do gradient steps\n",
    "for step in range(n_steps):\n",
    "    svi.step(data)\n",
    "    \n",
    "### Note that in the step() method we pass in the data, which then get passed to the model and guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The only thing we’re missing at this point is some data. So let’s create some data and assemble all the code snippets above into a complete script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................\n",
      "based on the data and our prior belief, the fairness of the coin is 0.540 +- 0.090\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import pyro\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI\n",
    "import pyro.distributions as dist\n",
    "\n",
    "# clear the param store in case we're in a REPL\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# create some data with 6 observed heads and 4 observed tails\n",
    "data = []\n",
    "for _ in range(6):\n",
    "    data.append(Variable(torch.ones(1)))\n",
    "for _ in range(4):\n",
    "    data.append(Variable(torch.zeros(1)))\n",
    "\n",
    "def model(data):\n",
    "    # define the hyperparameters that control the beta prior\n",
    "    alpha0 = Variable(torch.Tensor([10.0]))\n",
    "    beta0 = Variable(torch.Tensor([10.0]))\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.beta, alpha0, beta0)\n",
    "    # loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the bernoulli likelihood\n",
    "        pyro.observe(\"obs_{}\".format(i), dist.bernoulli, data[i], f)\n",
    "\n",
    "def guide(data):\n",
    "    # define the initial values of the two variational parameters\n",
    "    # we initialize the guide near the model prior (except a bit sharper)\n",
    "    log_alpha_q_0 = Variable(torch.Tensor([np.log(15.0)]), requires_grad=True)\n",
    "    log_beta_q_0 = Variable(torch.Tensor([np.log(15.0)]), requires_grad=True)\n",
    "    # register the two variational parameters with Pyro\n",
    "    log_alpha_q = pyro.param(\"log_alpha_q\", log_alpha_q_0)\n",
    "    log_beta_q = pyro.param(\"log_beta_q\", log_beta_q_0)\n",
    "    alpha_q, beta_q = torch.exp(log_alpha_q), torch.exp(log_beta_q)\n",
    "    # sample latent_fairness from Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness\", dist.beta, alpha_q, beta_q)\n",
    "\n",
    "# setup the optimizer\n",
    "adam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide, optimizer, loss=\"ELBO\", num_particles=7)\n",
    "\n",
    "n_steps = 4000\n",
    "# do gradient steps\n",
    "for step in range(n_steps):\n",
    "    svi.step(data)\n",
    "    if step % 100 == 0:\n",
    "        print('.', end='')\n",
    "\n",
    "# grab the learned variational parameters\n",
    "alpha_q = torch.exp(pyro.param(\"log_alpha_q\")).data.numpy()[0]\n",
    "beta_q = torch.exp(pyro.param(\"log_beta_q\")).data.numpy()[0]\n",
    "\n",
    "# here we use some facts about the beta distribution\n",
    "# compute the inferred mean of the coin's fairness\n",
    "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
    "# compute inferred standard deviation\n",
    "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
    "inferred_std = inferred_mean * np.sqrt(factor)\n",
    "\n",
    "print(\"\\nbased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
