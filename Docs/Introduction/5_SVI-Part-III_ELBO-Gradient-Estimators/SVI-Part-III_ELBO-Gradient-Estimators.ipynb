{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We’d like to maximize the log evidence logpθ(x) by maximizing the ELBO (the evidence lower bound) given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svi = SVI(model, guide, optimizer, \"ELBO\", trace_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Note that leveraging this dependency information takes extra computations, so trace_graph=True should only be invoked in the case where your model has non-reparameterizable random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines in Pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f9b0b19d1854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBaselineNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaselineNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# ... finish initialization ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class BaselineNN(nn.Module):\n",
    "    def __init__(self, dim_input, dim_hidden):\n",
    "        super(BaselineNN, self).__init__()\n",
    "        self.linear = nn.Linear(dim_input, dim_hidden)\n",
    "        # ... finish initialization ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.linear(x)\n",
    "        # ... do more computations ...\n",
    "        return baseline\n",
    "    \n",
    "def guide(x):  # here x is the current mini-batch of data\n",
    "    pyro.module(\"my_baseline\", baseline_module, tags=\"baseline\")\n",
    "    # ... other computations ...\n",
    "    z = pyro.sample(\"z\", dist.bernoulli, ...,\n",
    "                    baseline={'nn_baseline': baseline_module,\n",
    "                              'nn_baseline_input': x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A complete example with baselines\n",
    "    Recall that in the first SVI tutorial we considered a bernoulli-beta model for coin flips. Because the beta random variable is non-reparameterizable, the corresponding ELBO gradients are quite noisy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing inference with use_decaying_avg_baseline=True\n",
      "..................\n",
      "Did 1785 steps of inference.\n",
      "Final absolute errors for the two variational parameters (in log space) were 0.0500 & 0.0394\n",
      "Doing inference with use_decaying_avg_baseline=False\n",
      ".....................................\n",
      "Did 3635 steps of inference.\n",
      "Final absolute errors for the two variational parameters (in log space) were 0.0500 & 0.0305\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.optim as optim\n",
    "from pyro.infer import SVI\n",
    "import sys\n",
    "\n",
    "\n",
    "def param_abs_error(name, target):\n",
    "    return torch.sum(torch.abs(target - pyro.param(name))).data.numpy()[0]\n",
    "\n",
    "\n",
    "class BernoulliBetaExample(object):\n",
    "    def __init__(self):\n",
    "        # the two hyperparameters for the beta prior\n",
    "        self.alpha0 = Variable(torch.Tensor([10.0]))\n",
    "        self.beta0 = Variable(torch.Tensor([10.0]))\n",
    "        # the dataset consists of six 1s and four 0s\n",
    "        self.data = Variable(torch.zeros(10,1))\n",
    "        self.data[0:6, 0].data = torch.ones(6)\n",
    "        self.n_data = self.data.size(0)\n",
    "        # compute the alpha parameter of the exact beta posterior\n",
    "        self.alpha_n = self.alpha0 + self.data.sum()\n",
    "        # compute the beta parameter of the exact beta posterior\n",
    "        self.beta_n = self.beta0 - self.data.sum() + Variable(torch.Tensor([self.n_data]))\n",
    "        # for convenience compute the logs\n",
    "        self.log_alpha_n = torch.log(self.alpha_n)\n",
    "        self.log_beta_n = torch.log(self.beta_n)\n",
    "\n",
    "    def setup(self):\n",
    "        # initialize values of the two variational parameters\n",
    "        # set to be quite close to the true values\n",
    "        # so that the experiment doesn't take too long\n",
    "        self.log_alpha_q_0 = Variable(torch.Tensor([np.log(15.0)]), requires_grad=True)\n",
    "        self.log_beta_q_0 = Variable(torch.Tensor([np.log(15.0)]), requires_grad=True)\n",
    "\n",
    "    def model(self, use_decaying_avg_baseline):\n",
    "        # sample `latent_fairness` from the beta prior\n",
    "        f = pyro.sample(\"latent_fairness\", dist.beta, self.alpha0, self.beta0)\n",
    "        # use iarange to indicate that the observations are\n",
    "        # conditionally independent given f and get vectorization\n",
    "        with pyro.iarange(\"data_iarange\"):\n",
    "            # observe all ten datapoints using the bernoulli likelihood\n",
    "            pyro.observe(\"obs\", dist.bernoulli, self.data, f)\n",
    "\n",
    "    def guide(self, use_decaying_avg_baseline):\n",
    "        # register the two variational parameters with pyro\n",
    "        log_alpha_q = pyro.param(\"log_alpha_q\", self.log_alpha_q_0)\n",
    "        log_beta_q = pyro.param(\"log_beta_q\", self.log_beta_q_0)\n",
    "        alpha_q, beta_q = torch.exp(log_alpha_q), torch.exp(log_beta_q)\n",
    "        # sample f from the beta variational distribution\n",
    "        baseline_dict = {'use_decaying_avg_baseline': use_decaying_avg_baseline,\n",
    "                         'baseline_beta': 0.90}\n",
    "        # note that the baseline_dict specifies whether we're using\n",
    "        # decaying average baselines or not\n",
    "        pyro.sample(\"latent_fairness\", dist.beta, alpha_q, beta_q,\n",
    "                    baseline=baseline_dict)\n",
    "\n",
    "    def do_inference(self, use_decaying_avg_baseline, tolerance=0.05):\n",
    "        # clear the param store in case we're in a REPL\n",
    "        pyro.clear_param_store()\n",
    "        # initialize the variational parameters for this run\n",
    "        self.setup()\n",
    "        # setup the optimizer and the inference algorithm\n",
    "        optimizer = optim.Adam({\"lr\": .0008, \"betas\": (0.93, 0.999)})\n",
    "        svi = SVI(self.model, self.guide, optimizer, loss=\"ELBO\", trace_graph=True)\n",
    "        print(\"Doing inference with use_decaying_avg_baseline=%s\" % use_decaying_avg_baseline)\n",
    "\n",
    "        # do up to 10000 steps of inference\n",
    "        for k in range(10000):\n",
    "            svi.step(use_decaying_avg_baseline)\n",
    "            if k % 100 == 0:\n",
    "                print('.', end='')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            # compute the distance to the parameters of the true posterior\n",
    "            alpha_error = param_abs_error(\"log_alpha_q\", self.log_alpha_n)\n",
    "            beta_error = param_abs_error(\"log_beta_q\", self.log_beta_n)\n",
    "\n",
    "            # stop inference early if we're close to the true posterior\n",
    "            if alpha_error < tolerance and beta_error < tolerance:\n",
    "                break\n",
    "\n",
    "        print(\"\\nDid %d steps of inference.\" % k)\n",
    "        print((\"Final absolute errors for the two variational parameters \" +\n",
    "               \"(in log space) were %.4f & %.4f\") % (alpha_error, beta_error))\n",
    "\n",
    "# do the experiment\n",
    "bbe = BernoulliBetaExample()\n",
    "bbe.do_inference(use_decaying_avg_baseline=True)\n",
    "bbe.do_inference(use_decaying_avg_baseline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
