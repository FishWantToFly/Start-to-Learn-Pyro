{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Let’s first implement linear regression in PyTorch and learn point estimates for the parameters w and b. \n",
    "    Then we’ll see how to incorporate uncertainty into our estimates by using Pyro to implement Bayesian linear regression.\n",
    "    \n",
    "    y=wX+b+ϵ\n",
    "    ϵ represents observation noise\n",
    "    Mainly learn weight and bias\n",
    "    \n",
    "    prior: 先見條件\n",
    "    baysiean regression: exists prior (difference against linear regressiom)\n",
    "     \n",
    "    prior: depend on experience before, imagination about w and b 根據以前做的經驗，對於w, b的想像\n",
    "    posterior: prior after learning 學習完之後的prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "# for CI testing\n",
    "smoke_test = ('CI' in os.environ)\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We’ll generate a toy dataset with one feature and w=3 and b=1 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100  # size of toy data\n",
    "\n",
    "def build_linear_dataset(N, p=1, noise_std=0.01):\n",
    "    X = np.random.rand(N, p)\n",
    "    # w = 3\n",
    "    w = 3 * np.ones(p)\n",
    "    # b = 1\n",
    "    y = np.matmul(X, w) + np.repeat(1, N) + np.random.normal(0, noise_std, size=N)\n",
    "    y = y.reshape(N, 1)\n",
    "    # torch.tensor 要先變成tensor的形式\n",
    "    X, y = torch.tensor(X).type(torch.Tensor), torch.tensor(y).type(torch.Tensor)\n",
    "    data = torch.cat((X, y), 1)\n",
    "    assert data.shape == (N, p + 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        # p = number of features\n",
    "        super(RegressionModel, self).__init__()\n",
    "        \n",
    "        # return a model\n",
    "        self.linear = nn.Linear(p, 1)\n",
    "    \n",
    "    # 哪邊有 nonlinearity?\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# number of feature\n",
    "regression_model = RegressionModel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0050] loss: 27.8627\n",
      "[iteration 0100] loss: 12.8889\n",
      "[iteration 0150] loss: 5.1976\n",
      "[iteration 0200] loss: 1.7172\n",
      "[iteration 0250] loss: 0.4739\n",
      "[iteration 0300] loss: 0.1142\n",
      "[iteration 0350] loss: 0.0290\n",
      "[iteration 0400] loss: 0.0124\n",
      "[iteration 0450] loss: 0.0097\n",
      "[iteration 0500] loss: 0.0093\n",
      "[iteration 0550] loss: 0.0093\n",
      "[iteration 0600] loss: 0.0093\n",
      "[iteration 0650] loss: 0.0093\n",
      "[iteration 0700] loss: 0.0093\n",
      "[iteration 0750] loss: 0.0093\n",
      "[iteration 0800] loss: 0.0093\n",
      "[iteration 0850] loss: 0.0093\n",
      "[iteration 0900] loss: 0.0093\n",
      "[iteration 0950] loss: 0.0093\n",
      "[iteration 1000] loss: 0.0093\n",
      "Learned parameters:\n",
      "linear.weight: 3.005\n",
      "linear.bias: 0.998\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optim = torch.optim.Adam(regression_model.parameters(), lr=0.05)\n",
    "num_iterations = 1000 if not smoke_test else 2\n",
    "\n",
    "def main():\n",
    "    data = build_linear_dataset(N)\n",
    "    x_data = data[:, :-1]\n",
    "    y_data = data[:, -1]\n",
    "    for j in range(num_iterations):\n",
    "        # run the model forward on the data\n",
    "        y_pred = regression_model(x_data).squeeze(-1)\n",
    "        # calculate the mse loss\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "        # initialize gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        if (j + 1) % 50 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "    # Inspect learned parameters\n",
    "    print(\"Learned parameters:\")\n",
    "    for name, param in regression_model.named_parameters():\n",
    "        print(\"%s: %.3f\" % (name, param.data.numpy()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Regression\n",
    "\n",
    "    In order to do this, we’ll ‘lift’ the parameters w and b to random variables. We can do this in Pyro via random_module()\n",
    "    which effectively takes a given nn.Module and turns it into a distribution over the same module\n",
    "    \n",
    "    Need \"prior\" !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為什麼這邊是用二維的？\n",
    "# 1x1 --> 就是一個值\n",
    "loc = torch.zeros(1, 1)\n",
    "scale = torch.ones(1, 1)\n",
    "# define a unit normal prior\n",
    "prior = Normal(loc, scale)\n",
    "\n",
    "# overload the parameters in the regression module with samples from the prior\n",
    "lifted_module = pyro.random_module(\"regression_module\", regression_model, prior)\n",
    "# sample a regressor from the prior\n",
    "sampled_reg_model = lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    # Create unit normal priors over the parameters\n",
    "    # N(μ=0,σ=10)\n",
    "    loc, scale = torch.zeros(1, 1), 10 * torch.ones(1, 1)\n",
    "    bias_loc, bias_scale = torch.zeros(1), 10 * torch.ones(1)\n",
    "    w_prior = Normal(loc, scale).independent(1)\n",
    "    b_prior = Normal(bias_loc, bias_scale).independent(1)\n",
    "    priors = {'linear.weight': w_prior, 'linear.bias': b_prior}\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    # prior: p(z) regression_model: p(x/z)\n",
    "    # pyro.random_module is like a constructor\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, priors)\n",
    "    \n",
    "    # inference\n",
    "    # !!! sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    \n",
    "    with pyro.iarange(\"map\", N):\n",
    "        x_data = data[:, :-1]\n",
    "        y_data = data[:, -1]\n",
    "\n",
    "        # run the regressor forward conditioned on data\n",
    "        prediction_mean = lifted_reg_model(x_data).squeeze(-1)\n",
    "        \n",
    "        # condition on the observed data\n",
    "        pyro.sample(\"obs\",\n",
    "                    Normal(prediction_mean, 0.1 * torch.ones(data.size(0))),\n",
    "                    obs=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "# data 一定要寫，但這邊不會用到，標準格式\n",
    "def guide(data):\n",
    "    # define our variational parameters\n",
    "    w_loc = torch.randn(1, 1)\n",
    "    # note that we initialize our scales to be pretty narrow\n",
    "    w_log_sig = torch.tensor(-3.0 * torch.ones(1, 1) + 0.05 * torch.randn(1, 1))\n",
    "    b_loc = torch.randn(1)\n",
    "    b_log_sig = torch.tensor(-3.0 * torch.ones(1) + 0.05 * torch.randn(1))\n",
    "    # register learnable params in the param store\n",
    "    mw_param = pyro.param(\"guide_mean_weight\", w_loc)\n",
    "    sw_param = softplus(pyro.param(\"guide_log_scale_weight\", w_log_sig))\n",
    "    mb_param = pyro.param(\"guide_mean_bias\", b_loc)\n",
    "    sb_param = softplus(pyro.param(\"guide_log_scale_bias\", b_log_sig))\n",
    "    # guide distributions for w and b\n",
    "    w_dist = Normal(mw_param, sw_param).independent(1)\n",
    "    b_dist = Normal(mb_param, sb_param).independent(1)\n",
    "    dists = {'linear.weight': w_dist, 'linear.bias': b_dist}\n",
    "    # overload the parameters in the module with random samples\n",
    "    # from the guide distributions\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, dists)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.05})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 387.1771\n",
      "[iteration 0101] loss: 2.6335\n",
      "[iteration 0201] loss: 0.0786\n",
      "[iteration 0301] loss: -1.0272\n",
      "[iteration 0401] loss: -1.2048\n",
      "[iteration 0501] loss: -1.2476\n",
      "[iteration 0601] loss: -1.2592\n",
      "[iteration 0701] loss: -1.2531\n",
      "[iteration 0801] loss: -1.2487\n",
      "[iteration 0901] loss: -1.2363\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    pyro.clear_param_store()\n",
    "    data = build_linear_dataset(N)\n",
    "    for j in range(num_iterations):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss = svi.step(data)\n",
    "        if j % 100 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / float(N)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Baysiean regression學到了 w跟b 也是一種分布\n",
    "    學到w 跟 b 是從哪種distribution出來的\n",
    "    想像一條平面上有很多條線，sample就是取一條線"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[guide_log_scale_weight]: -3.780\n",
      "[guide_mean_weight]: 2.999\n",
      "[guide_log_scale_bias]: -4.149\n",
      "[guide_mean_bias]: 1.000\n"
     ]
    }
   ],
   "source": [
    "for name in pyro.get_param_store().get_all_param_names():\n",
    "    print(\"[%s]: %.3f\" % (name, pyro.param(name).data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0005512561183422804\n"
     ]
    }
   ],
   "source": [
    "X = np.linspace(6, 7, num=20)\n",
    "y = 3 * X + 1\n",
    "X, y = X.reshape((20, 1)), y.reshape((20, 1))\n",
    "x_data, y_data = torch.tensor(X).type(torch.Tensor), torch.tensor(y).type(torch.Tensor)\n",
    "loss = nn.MSELoss()\n",
    "y_preds = torch.zeros(20, 1)\n",
    "for i in range(20):\n",
    "    # guide does not require the data\n",
    "    sampled_reg_model = guide(None)\n",
    "    # run the regression model and add prediction to total\n",
    "    y_preds = y_preds + sampled_reg_model(x_data)\n",
    "# take the average of the predictions\n",
    "y_preds = y_preds / 20\n",
    "print (\"Loss: \", loss(y_preds, y_data).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
